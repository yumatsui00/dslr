正則化：過学習を抑えるために有効な手法で、道のデータに対する予測精度を上げることに用いられる。

高次元のデータや特徴量の多いデータを持つモデルは複雑になりがちで、その際のオーバーフィッティングを防ぐ

L１正則化：Lasso回帰：いらない変数を減らす。必要のない特徴量の重みを０に近づける。過学習を抑えるだけでなく、特徴選択を行える。
L2正則化：Ridge回帰：値の大きい係数にペナルティを与える。重みが大きいものの値を下げ、１つの変数の影響を下げられる。